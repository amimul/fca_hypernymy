{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pygraphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#path_to_dag = '/home/makrai/project/hypernym18-SemEval/top500words.dot'\n",
    "path_to_dag = '1A_UMBC_tokenized.txt_100_cbow.vec.gz_True_200_0.3_unit_True_vocabulary_filtered.alph.reduced2_more_permissive.dot'\n",
    "dag = nx.drawing.nx_agraph.read_dot(path_to_dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nx.write_gpickle(dag, '/home/makrai/project/hypernym18-SemEval/top500words.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deepest_occurrence = {}\n",
    "attributes_for_nodes = {}\n",
    "for n in dag.nodes(data=True):\n",
    "    words = n[1]['label'].split('|')[1].split('\\\\n')\n",
    "    node_id = int(n[1]['label'].split('|')[0])\n",
    "    attributes_for_nodes[node_id] = n[1]['label'].split('|')[2].split('\\\\n')\n",
    "    for w in words:\n",
    "        if not w in deepest_occurrence or deepest_occurrence[w][1] > len(words):\n",
    "            deepest_occurrence[w] = (node_id, len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64964, 2)\n",
      "['n198', 'n86', 'n89', 'n167', 'n180']\n",
      "(47845, 12)\n",
      "['n198', 'n80', 'n167', 'n180']\n"
     ]
    }
   ],
   "source": [
    "print(deepest_occurrence['dog'])\n",
    "print(attributes_for_nodes[deepest_occurrence['dog'][0]])\n",
    "print(deepest_occurrence['poodle'])\n",
    "print(attributes_for_nodes[deepest_occurrence['poodle'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "dog_location = deepest_occurrence['dog'][0]\n",
    "poodle_location = deepest_occurrence['poodle'][0]\n",
    "all_paths = nx.all_simple_paths(dag,'node{}'.format(dog_location), 'node{}'.format(poodle_location))\n",
    "print(list(all_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_file_names(dataset_dir, dataset_id):\n",
    "    dataset_mapping = {\n",
    "        '1A':['english', 'UMBC'],\n",
    "        '1B':['italian', 'it_itwac'],\n",
    "        '1C':['spanish', 'es_1Billion'],\n",
    "        '2A':['medical', 'med_pubmed'],\n",
    "        '2B':['music', 'music_bioreviews']\n",
    "    }\n",
    "    data_file = '{}/training/data/{}.{}.training.data.txt'.format(\n",
    "        dataset_dir,\n",
    "        dataset_id,\n",
    "        dataset_mapping[dataset_id][0]\n",
    "    )\n",
    "    gold_file = '{}/training/gold/{}.{}.training.gold.txt'.format(\n",
    "        dataset_dir,\n",
    "        dataset_id,\n",
    "        dataset_mapping[dataset_id][0]\n",
    "    )\n",
    "    vocab_file='{}/vocabulary/{}.{}.vocabulary.txt'.format(\n",
    "        dataset_dir,\n",
    "        dataset_id,\n",
    "        dataset_mapping[dataset_id][0]\n",
    "    )\n",
    "    return data_file, gold_file, vocab_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_dir = '/home/berend/datasets/semeval2018/SemEval18-Task9'\n",
    "dataset_id = path_to_dag[0:2]\n",
    "train_data_file, train_gold_file, vocab = generate_file_names(dataset_dir, dataset_id)\n",
    "dev_data_file, dev_gold_file = train_data_file.replace('training', 'trial'), train_gold_file.replace('training', 'trial')\n",
    "test_data_file = train_data_file.replace('training', 'test')\n",
    "\n",
    "train_queries = [l.split('\\t')[0].replace(' ', '_') for l in open(train_data_file)] # do we want to consider category as well?\n",
    "train_golds = [\n",
    "    [x.replace(' ', '_') for x in line.strip().split('\\t')] for line in open(train_gold_file)\n",
    "]\n",
    "\n",
    "dev_queries = [l.split('\\t')[0].replace(' ', '_') for l in open(dev_data_file)]\n",
    "dev_golds = [\n",
    "    [x.replace(' ', '_') for x in line.strip().split('\\t')] for line in open(dev_gold_file)\n",
    "]\n",
    "\n",
    "test_queries = [l.split('\\t')[0].replace(' ', '_') for l in open(test_data_file)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dag_reversed = dag.reverse()\n",
    "shortest_path_lengths = []\n",
    "hypernym_pairs_included = []\n",
    "paths = []\n",
    "for query, hypernyms in zip(train_queries, train_golds):\n",
    "    if query in deepest_occurrence:\n",
    "        query_location = deepest_occurrence[query][0]\n",
    "        for gold in hypernyms:\n",
    "            if not gold in deepest_occurrence:\n",
    "                continue\n",
    "            hypernym_pairs_included.append((query, gold))\n",
    "            gold_location = deepest_occurrence[gold][0]\n",
    "            if gold_location == query_location:\n",
    "                shortest_path_lengths.append(0)\n",
    "                paths.append([query_location])\n",
    "            else:\n",
    "                all_paths = list(nx.all_simple_paths(dag_reversed, 'node{}'.format(query_location), 'node{}'.format(gold_location)))\n",
    "                paths.append(all_paths)\n",
    "                if len(all_paths) == 0:\n",
    "                    shortest_path_lengths.append(-1)\n",
    "                else:\n",
    "                    shortest_path_lengths.append(min([len(p)-1 for p in all_paths]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({-1: 9977, 1: 202, 0: 184, 2: 183, 3: 85, 4: 7, 5: 2})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter([spl for spl in shortest_path_lengths]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py3k]",
   "language": "python",
   "name": "Python [py3k]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
