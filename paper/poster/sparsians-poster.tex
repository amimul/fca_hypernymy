% Berend+ 2018: 300-sparsans at SemEval-2018 Task 9

\documentclass{beamer}
%\usepackage[size=a0,debug]{beamerposter}
\usepackage[scale=1.2,size=a0,orientation=portrait,debug]{beamerposter}

\usepackage{graphicx}   % allows us to import images
\usepackage{booktabs}
%\usepackage{amsmath}
%\usepackage{multirow}
\usepackage[utf8]{inputenc}
\usepackage{t1enc}
%\usepackage{anyfontsize}
%\usepackage{authordate1-4}
%%\usepackage{multirow}
\usetheme{cpbgposter}
\usepackage[round]{natbib}
\usepackage{tikz}
\usepackage{url}
%\usepackage[magyar]{babel}
\usepackage{comment}
\usepackage{subfig}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Define the column width and poster size
% To set effective sepwid, onecolwid and twocolwid values, first choose how
% many columns you want and how much separation you want between columns
% The separation I chose is 0.024 and I want 4 columns
% Then set onecolwid to be (1-(4+1)*0.024)/4 = 0.22
% Set twocolwid to be 2*onecolwid + sepwid = 0.464
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newlength{\sepwid}
\newlength{\onecolwid}
\newlength{\twocolwid}
\setlength{\paperwidth}{48in} % hogy ne lógjon rá az utolsó oszlop a keretre
\setlength{\paperheight}{36in}
\setlength{\sepwid}{0.024\paperwidth}
\setlength{\onecolwid}{0.22\paperwidth}
\setlength{\twocolwid}{0.464\paperwidth}
%\setlength{\topmargin}{-0.5in}
%\usetheme{confposter}
%\usepackage{exscale} % scaling of the math extension font cmex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The next part fixes a problem with figure numbering. Thanks Nishan!
% When including a figure in your poster, be sure that the commands are typed
% in the following order:
% \begin{figure}
% \includegraphics[...]{...}
% \caption{...}
% \end{figure}
% That is, put the \caption after the \includegraphics
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usecaptiontemplate{
  \small
\structure{\insertcaptionnumber.~\insertcaptionname} \insertcaption}

% Define colours (see beamerthemeconfposter.sty to change these colour
% definitions)

% not working \definecolor{spartared}{rgb}{199,27,53}
% not working \definecolor{spartabrown}{rgb}{87,59,45}

\setbeamercolor{block title}{fg=dblue,bg=white}
%\setbeamercolor{block body}{fg=black,bg=white}
%\setbeamercolor{alerted title}{fg=white,bg=dblue}
%\setbeamercolor{block alerted title}{fg=white,bg=dblue!70}
%\setbeamercolor{block alerted body}{fg=black,bg=dblue!10}
\setbeamercolor{alerted text}{fg=szegedgreen}
%\setbeamercolor{item}{fg=dgreen}
\setbeamercolor{item projected}{bg=dgreen}


\newcommand{\bull}[1]{
  \begin{itemize}
    \item #1
  \end{itemize}
}

\author{Gábor Berend\inst{1}, Márton Makrai\inst{2}, and Péter Földiák\inst{3}}
\title{300-sparsans at SemEval-2018 Task 9: \\
Hypernymy as interaction of sparse attributes}
\institute{
  \inst{1} Department of Informatics, University of Szeged
\url{berendg@inf.u-szeged.hu} \and
\inst{2} Research Institute for Linguistics of the Hungarian Academy of
Sciences \url{makrai.marton@nytud.mta.hu} \and
\inst{3} Secret Sauce Partners \url{Peter.Foldiak@gmail.com}}

\begin{document}

\addtobeamertemplate{headline}{}
{
  \begin{tikzpicture}[remember picture,overlay]
    \node [shift={(-17 cm,-7cm)}]
    at (current page.north east)
    {\includegraphics[height=8cm]{SZTE}
    \hspace{1cm}
    \includegraphics[height=8cm]{../../../paper/Common/Logo/nytud/logo_only_tr.png}};
    % img//home/makrai/repo/paper/Common/Logo/nytud/.png}};
  \end{tikzpicture}

}

\newcommand{\ob}{\ensuremath{\mathcal O}}
\newcommand{\at}{\ensuremath{\mathcal A}}
\newcommand{\inci}{\ensuremath{\mathcal I}}
\newcommand{\oaconc}{\ensuremath{\langle O, A\rangle}}

\newenvironment{table*}%
{\begin{table}}
{\end{table}}

% \renewcommand{\resizebox}[3]{#3}
\renewcommand{\caption}[1]{}

\begin{frame}[t]
  \begin{columns}[t] % the [t] option aligns the column's content at the top

    \begin{column}{\sepwid} % empty spacer column
    \end{column}

    \begin{column}{\onecolwid} % 1st %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

      \begin{block}{Sparse word representations}

        \begin{itemize}
            %\item Natural language phenomena are extremely sparse
            % continuous word embeddings employ dense representations of words
            % Turning these dense representations into a much sparser form can
            % help in
          \item motivation
            \begin{itemize}
              \item focus on most salient parts of word representations \\
                \citep{Faruqui:2015,Berend:2017,Subramanian:2018}

                % Sparsity-based techniques often involve the coding of a large
                % number of signals over the same dictionary
                % \citep{Rubinstein:2008}
              \item  increase separability, interpretability
                \citep{Olshausen:1997} and stability against noise
                % (Donoho et al., 2006) the language domain begin no exception
                % \citep{Murphy:2012,Subramanian:2018}
            \end{itemize}

          \item \emph{Non-negative sparse coding}
            \begin{itemize}
              \item for interpretability \\
                \citep{Faruqui:2015,Fyshe:2015,Arora:2016} 

                \begin{quotation}
                  to describe the city of Pittsburgh, one might talk about phenomena
                  typical of the city, like erratic weather and large bridges. It is
                  redundant and inefficient to list negative properties, like the
                  absence of the Statue of Liberty 
                    \begin{flushright} \citep{Subramanian:2018}\end{flushright}
                \end{quotation}

                % in the language domain, where sparse features are interpreted as
                % lexical attributes, \cite{Berend:2016} shows the utitlity of
                % sparse word representations in low-resource setting,
              \item in word translation \citep{Berend:2018}
                \bull{sparse word vectors for the two languages such that \\ coding
                bases correspond to each other}
            \end{itemize}
        \end{itemize}
      \end{block}

      \begin{block}{Formal concept analysis (FCA)}
        \begin{itemize}
          \item FCA is the mathematization of a conceptual hierarchy
            \begin{itemize}
              \item a set of \emph{objects}, now words $w\in\ob$,
              \item a set of \emph{attributes}, now word vector indices $i\in\at$, and
              \item a binary incidence relation
                $\inci \subseteq \ob \times \at$, now \\ 
                $\langle w,i\rangle\in$ iff the $i$th coordinate in the sparse code
                of $w$ is \alert{non-zero}
            \end{itemize}
          \item FCA finds formal \emph{concepts}, pairs \oaconc,
            %~of object sets and attribute sets (
            $O\subseteq \ob,A \subseteq \at$, such that
            \begin{itemize}
              \item $A$ consists of the shared attributes of objects in $O$
                (and no more), and
              \item $O$ consists of
                the objects in $\ob$ that have all the attributes in $A$ (and
                no more)
              \item %(There is a closure-operator related to each FCA context, for which 
                $O$ and $A$ are closed sets iff \oaconc~is a concept
            \end{itemize}
          \item $O$ is called the extent and $A$ is the intent of the concept
          \item order defined in the context:
            if $\langle O_i , A_i \rangle$ are concepts in $C$, \\
            $\langle O_1 , A_1 \rangle$ is a \emph{subconcept} of $\langle
            O_2 , A_2 \rangle$ \\
            if $O_1 \subseteq O_2 $ which is equivalent to $A_1 \supseteq A_2 $
          \item lattice
          \item Adding attributes to \at,
            %will very probably grow the model.  However,
            the original concepts will be embedded as a substructure
            %in the larger lattice, with their ordering relationships preserved
          \item The smallest node in the concept lattice $n(w)$ whose extent contains a word
            $w$ is said to \emph{introduce} the object
          \item $h$ should be a hypernym of $q$ iff $n(q)\le n(h)$
          \item tools: \citet{Endres:2010,Cimiano:2005}
            %\url{www.compsens.uni-tuebingen.de/pub/pages/personals/3/concepts.py}
          \item features in the next column:%related to the \alert{FCA concept lattice}
            \begin{itemize}
              \item $n(w)$ is the concept that introduces $w$, i.e.~the most specific
                location within the DAG for $w$
                %, our features indicate whether $n(q)$
                % (1) coincides with that of $h$,
                % (2) is the parent (immediate successor) for that of $h$, or
                % (3) is the child (immediate predictions) for that of $h$

              \item $n_1\prec n_2$ denotes that $n_1$ is an immediate predecessor of $n_2$
              \item Parents, and even the inverse relation, proved to be more predictive
                than the conceptually motivated  $q\le h$
              \item not useful (see post-evaluation ablation experiments)
            \end{itemize}
        \end{itemize}
      \end{block}

      \begin{block}{The task and our results}
        \begin{itemize}
          \item extract hypernyms for query words \\ \cite{Camacho-Collados:2018}
          \item $(3\text{languages}+2)\times 3$ subtasks
            \begin{itemize}
              \item three languages, English, Italian, and Spanish 
              \item  + two domains, medical and music
              \item queries \alert{types}: concepts, entities, or all
            \end{itemize}
          \item Our system took first place in subtasks
            \begin{itemize}
              \item \emph{(1B) Italian} (\emph{all} and \emph{entities})
              \item \emph{(1C) Spanish entities} and
              \item \emph{(2B) music entities}
            \end{itemize}
        \end{itemize}
      \end{block}

    \end{column}

\begin{column}{\sepwid} % empty spacer column
\end{column}

%\begin{column}{\twocolwid} \begin{columns}
  \begin{column}{\onecolwid}

    \begin{block}{Sparse vectors}
      \begin{itemize}
        \item for each subtask, we solve for
          % Given the dense embedding matrix $W_x \in \mathbb{R}^{d \times \lvert
          % V_x \rvert}$, for some subcorpus of the shared task $x\in\{1A, 1B, 1C,
          % 2A, 2B\}$, where $\lvert V_x \rvert$ is the size of the vocabulary and
          % $d$ is set to 100. As a subsequent step, we turn $W_x$ into
          % \emph{sparse word vectors}
          \begin{equation}
            \min\limits_{D \in \mathcal{C}, \alpha \in
            \mathbb{R}_{\geq0}^{k\times |V|}} \lVert D\alpha - W_{x} \rVert_F + \lambda \lVert \alpha \rVert_1,
            \label{nonneg_SPAMS_objective}
          \end{equation}
        \item $\mathcal{C}$ is the convex set of $\mathbb{R}^{ d \times k}$
          matrices \\ with column norms $\le 1$, and
        \item $\alpha$ contains the sparse coefficients for the words
        \item akin to \citet{Berend:2017} + new non-negativity constraint over the
          elements of $\alpha$
        \item To keep the size of the FCA tree manageable, we only included the
          query words and the training hypernyms. This restriction turns out to be
          very useful.
        \item dense embedding $W$ unit-normed, $\lambda=.3$

      \end{itemize}
    \end{block}

    \begin{block}{Feaures summarized}{ for query $q$ and its hypernym candidate $h$ }
      \begin{table}
        \begin{tabular}{lc}
          \toprule
          \alert{dense vectors $W_x$}   & skip-gram in $d=100$-dimensions
          \\ \midrule
          {cosine}            & $\frac{\mathbf{q}^\intercal \mathbf{h}}{\lVert \mathbf{q} \rVert_2\lVert \mathbf{h} \rVert_2}$ \\ % 1
          {difference}        & $\lVert \mathbf{q} - \mathbf{h} \rVert_2$ \\ % 8
          {normRatio}         & $\frac{\lVert \mathbf{q}\rVert_2}{\lVert
          \mathbf{h} \rVert_2}$ \\ % 9
          \midrule 
          \alert{word strings}   & \\ 
          \midrule
          {qureyBeginsWith}   & $Q[0] = h$ \\ % 4
          {queryEndsWith}     & $Q[-1] = h$ \\
          {hasCommonWord} & $Q \cap H \neq \emptyset$ \\ % 5
          {sameFirstWord}        & $Q[0] = H[0]$ \\ % 6
          {sameLastWord}        & $Q[-1] = H[-1]$ \\
          {logFrequencyRatio} & $\log_{10}\frac{count(q)}{count(h)}$ \\ % 10
          {isFrequentHypernym}\footnotemark & $c \in MF_{50}(q.type)$\\
          \midrule 
          \alert{FCA} & see previous column    \\ 
          \midrule
          sameConcept & $n(h)=n(q)$ \\
          parent  & $n(q)\prec n(h)$ \\
          child  & $n(h)\prec n(q)$ \\
          \midrule 
          \alert{sparse vectors}   &  $\phi(w)$: set of non-zero coordinates,
          $k=200$  \\
          \midrule
          {overlappingBasis}  & $\phi(q) \cap \phi(h) \neq \emptyset$ \\ % 7
          {sparseDifference$_{q\setminus h}$} & $\lvert \phi(q) - \phi(h) \rvert$ \\ % 11
          {sparseDifference$_{h\setminus q}$} & $\lvert \phi(h) - \phi(q) \rvert$ \\
          attributePair$_{ij}$      & $\langle i,j\rangle\in\phi(q)\times\phi(h)$ \\ % 3.
          \bottomrule
        \end{tabular}
        \caption{The features employed in our classifier. \\
        }
        %The list of core features considering a $(q,h)$ pair of phrases.}
        \label{table:core_features}
      \end{table}

      % official submission ('concept', '
      %  cosines 8.8
      %  is_frequent_hypernym 2.8
      %  60_186 2.5
      %  cand_is_first_w -2.4
      %  textual_overlap 2.4
      %  ..
      % OFF OFF 1000
      %  ..
      %  same_first_w -1.0
      %  att_intersect 0.8
      %  diff_features -0.44
      %  norm_ratios 0.27
      %  ..
      %  freq_ratios_log 0.1
      %  att_diffB -0.06
      % OFF OFF 200 ('concept'
      %  ..
      %  same_first_w -1.0
      %  att_intersect 0.62
      %  diff_features -0.43
      %  norm_ratios 0.28
      %  ..
      %  freq_ratios_log 0.087
      %  att_diffB -0.03

\begin{itemize}
  \item $MF_{50}(q.type)$: 50 most frequent hypernyms for the query type
    (i.e.~concept or entity). Debugged after submission.
    % \item attributes $\phi(w)$: the set of basis vectors with non-zero weights
    % in the reconstruction of $\mathbf q$ and $\mathbf h$


  \item \texttt{attributePair}$_{ij}$s are the most important features
    \begin{itemize}
      \item indicator features for the interaction terms between the sparse
        coefficients in $\alpha$
        % Candidates get assigned with the Cartesian product derived from the
        % indices of the non-zero coefficients in $\alpha$
      \item This feature template induces $k^2$ features, with $k$ being the
        number of basis vectors introduced in the dictionary matrix $D$
        according to Eq.~\ref{nonneg_SPAMS_objective}
      \item the role of these features is similar to \emph{interaction terms}
        in regression
    \end{itemize}
\end{itemize}
\end{block}

  \begin{alertblock}{} \url{https://github.com/begab/fca_hypernymy} \end{alertblock} 

\end{column}

\begin{column}{\sepwid} % empty spacer column
\end{column}

\begin{column}{\onecolwid}



  \begin{block}{Implementation and tricks}
    \begin{itemize}
      \item \alert{dense vectors}: skip-gram (\cite{Mikolov:2013f}, $d=100$) trained
        for each sub-corpus provided by the organizers 
        % with the default training parameters of \texttt{word2vec} (w2v),
        % i.e.~a window size of 10 and 25 negative samples for each positive
        % context
      \item multi-token \alert{phrases} with the \texttt{word2phrase}
        software accompanying w2v
        % An additional source for identifying multi-token units in the training
        % corpora was the list of potential hypernyms released for each subtask
        % by the shared task organizers
        % a kettő hogy viszonyul egymáshoz?
      \item top 15 selected by \alert{logistic regression} trained for concepts
        and entities 
        \bull{\texttt{sklearn}
        \citep{Pedregosa:2011},%\footnote{\url{scikit-learn.org}},
        regularization parameter set to the default $1.0$}

      \item For each training pair $(q,h)$, we generated a number of \alert{negative
        samples} (i.e.~the training data does not include $h'$ as a valid hypernym for
        $q$)
        \bull{$h'$ sampled from the valid training hypernyms in  the query type
        (\textit{concept} or \textit{entity})}

      \item \alert{post-ranking} heuristic
        \begin{itemize}
          \item re-ranking according to background frequency in the training corpus
          \item motivation: more frequent words refer to more general concepts
            and more general hypernymy relations may be more easily to detect
        \end{itemize}
      \item {OOV backoff by query type}

        % \input{../baseline} 
        % \begin{itemize}
        % \item \alert{upper:} type-sensitive baseline, most frequent train
        % hypernym in the corresponding query type, concept or entity
        % \item \alert{lower:} the MFH baseline provided by the organizers (the
        % most frequent hypernyms in general)
        % \end{itemize}
  \end{itemize}
\end{block}

  \section{Results} \label{sec:results}

  \begin{block}{Two submissions}
    \begin{itemize}
        % TODO \item The sum of the two dimensions motivates our group name
      \item one of our submissions involved attribute pairs, the other not
      \item both submissions used the FCA-based features
        \bull{conceptually motivated but practically harmful}
    \end{itemize}

    % \input{../our_submissions}

    % \autoref{table:submissions} shows submission results.  The figures that
    % can be reproduced with the code in the project repo (\texttt{reprd}) is
    % slightly different from our official submissions (\texttt{offic}) for two
    % reasons: because the implementation of \texttt{isFreqHyp} contained a
    % bug, and because of the natural randomness in negative sampling.  For
    % reproducibility, we report result without the \texttt{isFreqHyp} feature
    % The randomness introduced by negative sampling is now factored out
    % by random seeding
  \end{block}
  \bigskip
      %\includegraphics[width=\columnwidth]{movie-poster.jpg}
  \begin{figure}
  %\includegraphics[\textwidth]{movie-poster}
  \end{figure}

  % TODO or not TODO
  %   \subsection{Feaure analysis} weights and histograms
  %   \subsection{Qualitative analysis} Here we investigate the system's
  %     ability to extract (good) hypernyms that were not seen for any word
  %     during training.  This setting is sometimes called zero-shot learning
  %   \subsection{Generality, frequency and vector length}



\end{column}

%\end{columns} \end{column}

\begin{column}{\sepwid} % empty spacer column
\end{column}

\begin{column}{\onecolwid} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  % \begin{block}{Vocabulary coverage}
  %
  % \input{../coverage}
  %
  % \end{block}

  \begin{block}{Post-evaluation analysis (without FCA)}

    Features derived with \alert{sparse attribute pairs} and/or \alert{FCA}:
    \input{../ablation1A}

    \begin{itemize}
      \item number of basis vectors in sparse coding ($k \in
        \{\mathbf{200}, 300, 1000\}$),
      \item number of \alert negative training \alert samples per positive sample
        \begin{itemize}
          \item submissions: \textbf{50} negative samples generated per query $q$
          \item post evaluation: all hypernyms in the training set
            \\ except for the proper hypernyms for $q$
        \end{itemize}
      \item candidates filtered to those present in the training data
        \begin{itemize}
          \item Historically, applied to speed up the FCA algorithm
            (smaller concept lattice)
        \end{itemize}
      \item \textbf{boldface font} above: submission settings
    \end{itemize}

  \end{block}
  \begin{block}{1A}
    \input{../big_table}

    % Coverage with candidate filtering:
    %
    % \input{../upper}

    %  \end{block}
    %
    % \begin{block}{Discussion}
    %
    % \begin{itemize}
    % \item it is advantageous to apply sparse representation of more
    % expressive power (i.e.~a higher number of basis vectors)
    % \item Generating more negative samples also provides some additional
    % performance boost
    % \item longer sparse vector help irrespective whether candidate
    % filtering is employed or not
    % \bull{effect more pronounced when hypernym candidates are not
    % filtered}
    % \end{itemize}

    \end{block}
      \begin{block}{All the subtasks}

        \input{../post_eval}

        \begin{itemize}
          \item upper: our system, ($k = 1000, ns = 50$, hypernym candidate
            filtering on, FCA off) 
          \item lower: subtask winner, official scores 
        \end{itemize}

        % \autoref{table:post_eval}. It can be seen from these enhanced results
        % for all types (concepts and entities mixed) that
        % we would win (1B) Italian and (1C) Spanish. Our post-evaluation
        % system -- which only differs from our participating system that it
        % fixes the calculation of a features, does not rely on FCA-based
        % features and uses $k=1000$ -- would also place third in the rest of
        % the subtasks

      \end{block}

  \begin{block}{Future work: hierarchical sparse coding}
    \begin{itemize}
      \item trees describe the order in which variables “enter the model”
        (i.e., take non-zero values, \cite{Zhao:2009})
      \item a node may enter only if its ancestors also do
      \item % the dimensions that correspond to
        top level nodes should focus on \emph{general} meaning components
        % that are present in most words
      \item efficient implementation \citep{Yogatama:2015}
      \item correspondence between
        the variable tree and the hypernym hierarchy
    \end{itemize}

        {\footnotesize Research supported by the project \emph{Integrated
        program for training new generation of scientists in the fields of computer
        science}, no.~EFOP-3.6.3-VEKOP-16-2017-0002. The project has been supported
        by the European Union and co-funded by the European Social Fund.}
  \end{block}
\end{column} \end{columns} \end{frame}

  \begin{block}{References}
    \footnotesize
    %\fontsize{26}{32}
    \bibliographystyle{abbrvnat}
    \bibliography{../../../paper/Common/Bib/ml}
  \end{block}
\end{document}

