%
% File naaclhlt2018.tex
%
%% Based on the style files for NAACL-HLT 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article} 
\usepackage{lmodern}
\usepackage[T1]{fontenc} 
\usepackage[utf8]{inputenc}
\usepackage[hyperref]{naaclhlt2018}
\usepackage{times}
\usepackage{latexsym}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
%\usepackage{amsmath}

\aclfinalcopy % Uncomment this line for all SemEval submissions

\setlength\titlebox{3in}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

%Title format for system description papers by task participants
\title{[300-sparsians] at SemEval-2018 Task 9: Employing Sparse Word Representations for Hypernym Discovery}
% TODO Sparse Word Vectors and Concept Lattices for ...
%Title format for task description papers by task organizers
%\title{SemEval-2018 Task [TaskNumber]:  [Task Name]}

\author{Gábor Berend \\
Department of Informatics \\ University of Szeged \\
Árpád tér 2, H6720 Szeged, Hungary \\
{\tt berendg@inf.u-szeged.hu} \\\And
  Márton Makrai  \\ % TODO sorrend?
  Institute for Linguistics \\
  Hungarian Academy of Sciences \\
  Benczúr u. 33, H1068 Budapest, Hungary \\
  {\tt makrai.marton@nytud.mta.hu} \\\AND
  Péter Földiák \\
  Secret Sauce Partners, \\
  657 Mission Suite 410, \\ 
  San Francisco CA 94105 \\
  {\tt email@domain} \\} % TODO

\date{}

\begin{document}

% Tudom, hogy hatékonyabb alulról fölfelé írni egy cikket, de nekem ez a
% mániám, hogy le kell írnonom néhány hangzatos kezdőmondatot, hogy beállítsam
% a hangulatomat.
\maketitle

\vspace{1cm}

\begin{abstract}
  We apply a concept hierarchy obtained from a word embedding by combining the
  methods of sparse coding and formal concept analysis (concept lattices).
  Our experiments place first in four subtasks (e.g.~the music entities
  subtask).
\end{abstract}

\section{Introduction}

The idea of acquiring concept hierarchies from a text corpus with the tools of
Formal Concept Analysis (FCA) is relatively new \citep{Cimiano:2005}. Here we
combine this approach with sparse word representations \citep{Faruqui:2015},
thus indirectly exploiting successful word embedding techniques
\citep{Mikolov:2013d}.

\dots % TODO

Natural language phenomena are extremely sparse by their nature, whereas
continuous word embeddings employ dense representations of words. Turning
these dense representations into a much sparser form can help in focusing on
most salient parts of word representations \citep{Faruqui:2015,Berend:2016}.

\newcommand{\ob}{\ensuremath{\mathcal O}}
\newcommand{\at}{\ensuremath{\mathcal A}}
\newcommand{\inci}{\ensuremath{\mathcal I}}
\newcommand{\oaconc}{\ensuremath{\langle O, A\rangle}}

\ensuremath{\mathcal o}

Formal Concept Analysis (FCA) is the mathematization of \emph{concept} and conceptual
hierarchy \citep{Ganter:1999,Endres:2008}. % vagy 2009
An FCA context $C = \langle \ob, \at, \inci\rangle$ is comprised of
a set of \emph{objects} $\ob$, a set of \emph{attributes} $\at$ and
a binary incidency relation $\inci \subseteq \ob \times \at$ 
between members of \ob and \at.
In our application, the members of $\ob$ are words, whereas
the members of $\at$ are the sparse coding coordinate indices.
Relation $\inci$ contains a pair $\langle o, a\rangle$
if the $a$th coordinate of the sparse vector for word $o \in \ob$ is non-zero,
% It is customary to represent the context as a cross table, where the
% row(column) headings are the object(attribute) names.  For each pair (g, m) ∈
% \inci, the corresponding cell in the cross table has an “×”.
% TODO example
We define the prime operator $'$ both for objects and attributes in a dual way:
given $O\subseteq \ob$ and $A \subseteq \at$,
$O'$ is defined as $\{ a\in \at\mid \forall o\in O, \langle o,a \rangle \in \inci\}$
i.e.~$O'$ contains the shared attributes of objects in $O$; and
$A'$ is defined as $\{ o\in \ob\mid \forall a\in A, \langle o,a \rangle \in \inci\}$
i.e.~$A'$ contains the objects in $\ob$ that have all the attributes in $A$.
A formal concept in a context $C$ is a pair \oaconc~ such that $O' = A$ and $A' = O$.
$O$ is called the extent and $A$ is the intent of the concept.\footnote{
  %IB(C) denotes the set of all concepts of the context C.
  Those who are familiar with closure operators may note that the double
  application of $'$ is a closure operation both on objects and attributes: with
  notation $\bar S=S''$, for either $S\subseteq \ob$ or $S\subseteq \at$ we have
  $S\subseteq \bar S$ and $\bar{\bar S}=S$, and the following conditions are
  equivalent for all $O\subseteq \ob$ and $A\subseteq \at$:
  \begin{itemize}
    \item \oaconc~is a concept
    \item $O$ is a closed set 
      %with respect to $O\mapsto\bar O$
      and $A=O'$
    \item $A$ is a closed set 
      %with respect to $A\mapsto\bar A$ 
      and $O=A'$
  \end{itemize}
}
For a lattice representation of the relationships between concepts, one defines
an order on $C$:
if $\langle A_1 , B_1 \rangle$ and $\langle A_2 , B_2 \rangle$ are concepts in
$C$, $\langle A_1 , B_1 \rangle$ is a \emph{subconcept} of $\langle A_2 , B_2
\rangle$ if $A_1 \subseteq A_2 $ which is equivalent to $B_1 \supseteq B_2 $.  In this case
%, $\langle A_2 , B_2 \rangle$ is a \emph{superconcept} of $\langle A_1 , B_1 \rangle$
%and
we write $\langle A_1 , B_1 \rangle \le \langle A_2 , B_2 \rangle$.
% The relation $\le$ is called the \emph{order} of the concepts.

$C$ and the concept order form a %complete
lattice.  The concept lattice of the context in Table 1, with full and reduced
% TODO figure
% In-Close2 draws only reduced if I'm not mistaken.
labelling, is shown in \ref{fig:monkeylattice}.
Full labelling means that a concept node is depicted with its full extent and
intent. A reduced labelled concept lattice shows an object only in the smallest
(w.r.t. $\le$) concept of whose extent the object is a member.
% This concept is called the object concept, or the concept that introduces the
% object.
Likewise, an attribute is shown only in the largest concept of whose intent the
attribute is a member.%, the attribute concept, which introduces the attribute.
% The closedness of extents and intents has an important consequence for
% neuroscientific applications. Adding attributes to \at (e.g. responses of
% additional neurons) will very probably grow IB(C). However, the original
% concepts will be embedded as a substructure in the larger lattice, with their
% ordering relationships preserved.

For further details of the shared task see
\cite{semeval2018task9}.

Our source code in available at \url{https://github.com/begab/fca_hypernymy}.

%\section{Related work} bele fog ez férni 4 oldalba? ha kevés lesz a hely, akkor ezen lehet fogni

\section{Our methodology}
We used the popular skip-gram (SG) and continuous-bag-of-words (CBOW) approaches \cite{DBLP:journals/corr/abs-1301-3781} to train $d=100$ dimensional dense distributed word representations for each sub-corpora. For some subcorpus $x$, we denote the embedding matrix as $W_x \in \mathbb{R}^{\lvert V_x \rvert \times d}$ with $\lvert V_x \rvert$ denoting the size of the vocabulary and $d$ is set to 100 as stated previously.

As a subsequent step we turn the dense vectorial word representations into sparse word vectors akin to \citet{TACL1063} by solving for
\begin{equation}
\min\limits_{D \in \mathcal{C}, \alpha \in \mathbb{R}_{\geq0}} \lVert D\alpha - W \rVert_F + \lambda \lVert \alpha \rVert_1,
\label{nonneg_SPAMS_objective}
\end{equation}
where $\mathcal{C}$ is the convex set of matrices containing only such vectors whose norm does not exceeds $1$ and $\alpha$ contains the sparse coefficients encoding which basis vectors from $D$ takes part in the reconstruction of the vectorial representation of some element of the vocabulary. The only difference compared to \cite{TACL1063} is that here we ensure a non-negativity constraint over the elements of $\alpha$.

For the elements of the vocabulary we ran the formal concept analysis tool of \citet{2010378} \footnote{\url{www.compsens.uni-tuebingen.de/pub/pages/personals/3/concepts.py}}.

\section{Experimental results}

\section{Conclusion}

%\bibliography{semeval2018}%,../../paper/Common/Bib/ml}
\bibliographystyle{acl_natbib}

\end{document}
