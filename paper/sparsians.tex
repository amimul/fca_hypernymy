%% Based on the style files for NAACL-HLT 2018, which were

\documentclass[11pt,a4paper]{article}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

% To debug "\pdfendlink ended up in different nesting level than \pdfstartlink"
\usepackage[nohyperref]{naaclhlt2018}
\usepackage[draft]{hyperref}
%\usepackage{hyperref}

\usepackage{times}
\usepackage{latexsym}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
%\usepackage{amsmath}

\aclfinalcopy % Uncomment this line for all SemEval submissions

\setlength\titlebox{3in}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\title{[300-sparsians] at SemEval-2018 Task 9: \\
Employing Sparse Word Representations for Hypernym Discovery}
% TODO dorp "Employing"

\author{Gábor Berend \\
Department of Informatics \\ University of Szeged \\
Árpád tér 2, H6720 Szeged, Hungary \\
{\tt berendg@inf.u-szeged.hu} \\\And
  Márton Makrai  \\
  Institute for Linguistics \\
  Hungarian Academy of Sciences \\
  Benczúr u. 33, H1068 Budapest, Hungary \\
  {\tt makrai.marton@nytud.mta.hu} \\\AND
  Péter Földiák \\
  Secret Sauce Partners, \\
  657 Mission Suite 410, \\
  San Francisco CA 94105 \\
  {\tt Peter.Foldiak@gmail.com} \\}

\date{}

\begin{document}

\maketitle

\begin{abstract}
  We apply a concept hierarchy obtained from a word embedding by combining the
  methods of sparse coding and formal concept analysis (concept lattices).
  Our experiments place first in four subtasks (e.g.~the music entities
  subtask).
\end{abstract}

% bázisok alapján épített fca-val akartuk megoldani az egészet, de aztán azt
% láttuk h ez önmagában nem lenne elég, mert a bázisok nem viselkedtek olyan
% szépen, mint amennyire kellett volna nekik ahhoz, hogy az fca-ból
% kipottyanjon a fogalmi hierarchia

% a potenciális hipernímákat jelentősen meg kellett szűrni ahhoz h az fca-n
% gyorsítani tudjunk ezáltal viszont a hipernímaként való rangsorolás
% tekintetében is egy szűkített halmazzal dolgoztunk a szervezők által
% kiadottakhoz képest
% ami miatt 0.5 körüli MAP-nál jobbat elvileg sem érhettünk volna el

% \item sűrű vektoros és felszíni jegyek
% \item ritka reprezentációból származtatott descartes szorzatos jegyeket
% \item ablációs kísérletek erre (descaretes on/off)
% \item a tanult súlyokról lehetne még írni

\section{Introduction}

Natural language phenomena are extremely sparse by their nature, whereas
continuous word embeddings employ dense representations of words. Turning
these dense representations into a much sparser form can help in focusing on
most salient parts of word representations
\citep{Faruqui:2015,Berend:2017,Subramanian:2018}.

Sparse representation is related to hypernymy in natural ways.
One formulation is hierarchical sparse coding \citep{Zhao:2009}, where trees
describe the order in which variables “enter the model” (i.e., take nonzero
values). A node may take a nonzero value only if its ancestors also do: the
dimensions that correspond to top level nodes should focus on “general”
contexts that are present in most words. \citep{Yogatama:2015} offer an
implementation that is efficient for gigaword corpora.

Here we employ sparse coding for hypernym extraction in a different way, namely
through Formal Concept Analysis (FCA).  

nonneg \citep{Faruqui:2015,Fyshe:2015,Danish:2016}
%                                     Danish, Dahiya, and Talukdar

entities, music, Spanish

a task kiírásában levő irodalom

For further details of the shared task see \cite{Camacho-Collados:2018}.

Our source code in available at \url{https://github.com/begab/fca_hypernymy}.

\subsection{Formal concept analysis}

\newcommand{\ob}{\ensuremath{\mathcal O}}
\newcommand{\at}{\ensuremath{\mathcal A}}
\newcommand{\inci}{\ensuremath{\mathcal I}}
\newcommand{\oaconc}{\ensuremath{\langle O, A\rangle}}

Formal Concept Analysis (FCA) is the mathematization of \emph{concept} and conceptual
hierarchy \citep{Ganter:2012,Endres:2010}. % vagy 2009
An FCA context $C = \langle \ob, \at, \inci\rangle$ is comprised of
a set of \emph{objects} $\ob$, a set of \emph{attributes} $\at$ and
a binary incidency relation $\inci \subseteq \ob \times \at$
between members of \ob and \at.
In our application, the members of $\ob$ are words, whereas
the members of $\at$ are the sparse coding coordinate indices.
Relation $\inci$ contains a pair $\langle o, a\rangle$
if the $a$th coordinate of the sparse vector for word $o \in \ob$ is non-zero,
% It is customary to represent the context as a cross table, where the
% row(column) headings are the object(attribute) names.  For each pair (g, m) ∈
% \inci, the corresponding cell in the cross table has an “×”.
% TODO example
We define the prime operator $'$ both for objects and attributes in a dual way:
given $O\subseteq \ob$ and $A \subseteq \at$,
$O'$ is defined as $\{ a\in \at\mid \forall o\in O, \langle o,a \rangle \in \inci\}$
i.e.~$O'$ contains the shared attributes of objects in $O$; and
$A'$ is defined as $\{ o\in \ob\mid \forall a\in A, \langle o,a \rangle \in \inci\}$
i.e.~$A'$ contains the objects in $\ob$ that have all the attributes in $A$.
A formal concept in a context $C$ is a pair \oaconc~ such that $O' = A$ and $A' = O$.
$O$ is called the extent and $A$ is the intent of the concept.\footnote{
  %IB(C) denotes the set of all concepts of the context C.
  Those who are familiar with closure operators may note that the double
  application of $'$ is a closure operation both on objects and attributes: with
  notation $\bar S=S''$, for either $S\subseteq \ob$ or $S\subseteq \at$ we have
  $S\subseteq \bar S$ and $\bar{\bar S}=S$, and the following conditions are
  equivalent for all $O\subseteq \ob$ and $A\subseteq \at$:
  \begin{itemize}
    \item \oaconc~is a concept
    \item $O$ is a closed set
      %with respect to $O\mapsto\bar O$
      and $A=O'$
    \item $A$ is a closed set
      %with respect to $A\mapsto\bar A$
      and $O=A'$
  \end{itemize}
}
For a lattice representation of the relationships between concepts, one defines
an order on $C$:
if $\langle A_1 , B_1 \rangle$ and $\langle A_2 , B_2 \rangle$ are concepts in
$C$, $\langle A_1 , B_1 \rangle$ is a \emph{subconcept} of $\langle A_2 , B_2
\rangle$ if $A_1 \subseteq A_2 $ which is equivalent to $B_1 \supseteq B_2 $.  In this case
%, $\langle A_2 , B_2 \rangle$ is a \emph{superconcept} of $\langle A_1 , B_1 \rangle$
%and
we write $\langle A_1 , B_1 \rangle \le \langle A_2 , B_2 \rangle$.
% The relation $\le$ is called the \emph{order} of the concepts.

$C$ and the concept order form a %complete
lattice.  The concept lattice of the context in Table 1, with full and reduced
% TODO figure
% In-Close2 draws only reduced if I'm not mistaken.
labelling, is shown in \ref{fig:monkeylattice}.
Full labelling means that a concept node is depicted with its full extent and
intent. A reduced labelled concept lattice shows an object only in the smallest
(w.r.t. $\le$) concept of whose extent the object is a member.
% This concept is called the object concept, or the concept that introduces the
% object.
Likewise, an attribute is shown only in the largest concept of whose intent the
attribute is a member.%, the attribute concept, which introduces the attribute.
% The closedness of extents and intents has an important consequence for
% neuroscientific applications. Adding attributes to \at (e.g. responses of
% additional neurons) will very probably grow IB(C). However, the original
% concepts will be embedded as a substructure in the larger lattice, with their
% ordering relationships preserved.

\section{Related work}

The idea of acquiring concept hierarchies from a text corpus with the tools of
Formal Concept Analysis (FCA) is relatively new \citep{Cimiano:2005}.

\section{Our submissions}

We used the popular skip-gram (SG) and continuous-bag-of-words (CBOW)
approaches \cite{Mikolov:2013f} to train $d=100$ dimensional dense distributed
word representations for each sub-corpora. For some subcorpus $x$, we denote
the embedding matrix as $W_x \in \mathbb{R}^{\lvert V_x \rvert \times d}$ with
$\lvert V_x \rvert$ denoting the size of the vocabulary and $d$ is set to 100
as stated previously.

As a subsequent step we turn the dense vectorial word representations into
sparse word vectors akin to \citet{Berend:2017} by solving for
\begin{equation}
\min\limits_{D \in \mathcal{C}, \alpha \in \mathbb{R}_{\geq0}} \lVert D\alpha - W \rVert_F + \lambda \lVert \alpha \rVert_1,
\label{nonneg_SPAMS_objective}
\end{equation}
where $\mathcal{C}$ is the convex set of matrices containing only such vectors
whose norm does not exceeds $1$ and $\alpha$ contains the sparse coefficients
encoding which basis vectors from $D$ takes part in the reconstruction of the
vectorial representation of some element of the vocabulary. The only difference
compared to \cite{Berend:2017} is that here we ensure a non-negativity
constraint over the elements of $\alpha$.

For the elements of the vocabulary we ran the formal concept analysis tool of
\citet{Endres:2010}\footnote{\url{www.compsens.uni-tuebingen.de/pub/pages/personals/3/concepts.py}}.

% why 300 in the group name (akár ott, hogy 300-as ritkával jobb is lehetett
% volna, vagy ott, hogy 200 + 100 -- de ha magától értődő, akkor
% ne)

\section{Experimental results}

\subsection{Restricted cantdidates}

Table~\ref{table:upper} lists the best possible results we can achieve.

\begin{table}
	\centering
	\begin{tabular}{c|ccc}
		Subtask & MAP & MRR & P@1 \\ \hline
		1A & 0.542 & 0.748 & 0.670 \\
		1B & 0.596 & 0.841 & 0.796 \\
		1C & 0.624 & 0.839 & 0.776 \\
		2A & 0.478 & 0.726 & 0.652 \\
		2B & 0.459 & 0.744 & 0.650 \\
	\end{tabular}
	\label{table:upper}
	\caption{Best possible results obtainable on the test set for our
submissions due to the filtration of the possible set of gold hypernyms
we apply in order to speed up the FCA algorithm we use.}
\end{table}

a metrikáknak nagy a szórása. többszöri futtatás %TODO

entity length ratio figure %TODO

\subsection{All provided cantdidates}

weights of dense and surface features %TODO

\subsection{Qualitative analysis} %TODO ?

entity music, Spannish (MRR, p@1)

\section{Conclusion}

\bibliography{../../paper/Common/Bib/ml}
\bibliographystyle{acl_natbib}

\end{document}
