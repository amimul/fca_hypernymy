%% Based on the style files for NAACL-HLT 2018

\documentclass[11pt,a4paper]{article}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

% To debug "\pdfendlink ended up in different nesting level than \pdfstartlink"
\usepackage[nohyperref]{naaclhlt2018}
%\usepackage[draft]{hyperref}
\usepackage{hyperref}

\usepackage{times}
\usepackage{latexsym}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{verbatim}

\aclfinalcopy % Uncomment this line for all SemEval submissions

\setlength\titlebox{3in}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\title{300-sparsians at SemEval-2018 Task 9: \\
Hypernymy as interaction of sparse attributes}

\author{Gábor Berend \\
Department of Informatics \\ University of Szeged \\
Árpád tér 2, H6720 Szeged, Hungary \\
{\tt berendg@inf.u-szeged.hu} \\\And
  Márton Makrai  \\
  Institute for Linguistics \\
  Hungarian Academy of Sciences \\
  Benczúr u. 33, H1068 Budapest, Hungary \\
  {\tt makrai.marton@nytud.mta.hu} \\\AND
  Péter Földiák \\
  Secret Sauce Partners \\
  657 Mission Suite 410, \\
  San Francisco CA 94105 \\
  {\tt Peter.Foldiak@gmail.com} \\}

\date{}

\begin{document}

\maketitle

\begin{abstract}
  This paper describes 300-sparsians’s participation in SemEval-2018 Task 9:
  \emph{Hypernym Discovery}, with a system based on sparse coding and a formal
  concept hierarchy obtained from word embeddings.
  Our system took first place in subtasks
  \emph{(1B) Italian} (\emph{all} and \emph{entities}),
  \emph{(1C) Spanish entities}, and
  \emph{(2B) music entities}.
\end{abstract}


\section{Introduction}

Natural language phenomena are extremely sparse by their nature,
whereas continuous word embeddings employ dense representations of words.
Turning these dense representations into a much sparser form can help in
focusing on most salient parts of word representations
\citep{Faruqui:2015,Berend:2017,Subramanian:2018}.

%\citet{Bengio:2013} discusses,
%\begin{quotation}
%  [In vision,] Coates and Ng (2011a) demonstrated on the CIFAR-10 object
%  classification task (Krizhevsky and Hinton, 2009) with a patch-base feature
%  extraction pipeline, that in the regime with few labeled training examples
%  per class, the sparse coding representation significantly outperformed other
%  highly competitive encoding schemes. [\dots] There are numerous examples of its
%  successful application as a feature representation scheme, including [\dots]
%  NLP (Bagnell and Bradley, 2009) as well as being a very successful model of
%  the early visual cortex (Olshausen and Field, 1996). Sparsity criteria can
%  also be generalized successfully to yield groups of features that prefer to
%  all be zero, but if one or a few of them are active then the penalty for
%  activating others in the group is small. Different group sparsity patterns
%  can incorporate different forms of prior knowledge (Kavukcuoglu et al., 2009;
%  Jenatton et al., 2009; Bach et al., 2011; Gregor et al., 2011).
%\end{quotation}

Sparsity-based techniques often involve the coding of a large number of signals
over the same dictionary \citep{Rubinstein:2008}. Sparse, overcomplete
representations have been motivated in various domains as a way to increase
separability and interpretability \citep{Olshausen:1997} and
% Lewicki and Sejnowski, 2000)
stability in the presence of noise.
%(Donoho et al., 2006)
%the language domain begin no exception \citep{Murphy:2012,Subramanian:2018}.

\emph{Non-negativity} has also been argued to be adventageous for interpretability
\citep{Faruqui:2015,Fyshe:2015,Arora:2016}. As \citet{Subramanian:2018}
illustrates this in the language domain, where sparse features are interpreted
as lexical attributes, ``to describe the city of Pittsburgh, one might talk
about phenomena typical of the city, like erratic weather and large bridges. It
is redundant and inefficient to list negative properties, like the absence of
the Statue of Liberty''.
% \cite{Berend:2016} shows the utitlity of sparse word representations in
% low-resource setting,
\citet{Berend:2018} utilizes non-negative sparse coding for word translation by
training sparse word vectors for the two languages such that coding bases
correspond to each other.

Here we apply sparse feature pairs to hypernym extraction. The role of an
attribute pair $\langle i,j\rangle\in\phi(q)\times\phi(h)$ (where $q$ is the query word,
$h$ is the hypernym candidate, and $\phi(w)$ is the index of a nonzero
% TODO nonzero vs nonzero vs nonzero legvégül egységesít
% bas[ie]s, attribute, feature
component in the sparse representations of $w$) is similar to \emph{interaction
terms} in regression.

Sparse representation is related to hypernymy in various natural ways.  One of
them is through \emph{Formal concept Analysis (FCA)}.  The idea of acquiring concept
hierarchies from a text corpus with the tools of Formal concept Analysis (FCA)
is relatively new \citep{Cimiano:2005}.
Our submissions experiment with formal concept analysis tool by
\citet{Endres:2010}. See the next section for a describtion of formal concept
lattices, and how hypernymys can be found in them.

Another natural formulation is related to hierarchical sparse coding
\citep{Zhao:2009}, where trees describe the order in which variables “enter the
model” (i.e., take nonzero values). A node may take a nonzero value only if its
ancestors also do: the dimensions that correspond to top level nodes should
focus on “general” contexts that are present in most words.
\citet{Yogatama:2015} offer an implementation that is efficient for gigaword
corpora. Exploiting the correspondence between the variable tree and the
hypernym hierarchy offers itself as a natural chioice.

The task \citep{Camacho-Collados:2018} evaluated systems at their ability to
extract hypernyms for query word in five subtasks (three languages, English,
Italian, and Spanish, and two domains, medical and music. Queries have been
categprized as concepts or entities. Result have been proclaimed in separately
in the two categories and in overall as well, thus resulting in 15
combinations.
Our system took first place in the music entities subtasks (2B).
% TODO Spanish
Detailed results for our system appear in \autoref{sec:results}.
Our source code in available online\footnote{\url{https://github.com/begab/fca_hypernymy}}.

% Here we illustrate the effectiveness of sparse coding derived word
% representations in the task of hypernymy extraction.

\subsection{Formal concept analysis}

\newcommand{\ob}{\ensuremath{\mathcal O}}
\newcommand{\at}{\ensuremath{\mathcal A}}
\newcommand{\inci}{\ensuremath{\mathcal I}}
\newcommand{\oaconc}{\ensuremath{\langle O, A\rangle}}

Formal concept Analysis (FCA) is the mathematization of \emph{concept} and conceptual
hierarchy \citep{Ganter:2012,Endres:2010}. % vagy 2009
An FCA context %$C = \langle \ob, \at, \inci\rangle$
comprises of
a set of \emph{objects} $\ob$, a set of \emph{attributes} $\at$ and
a binary incidency relation $\inci \subseteq \ob \times \at$
between members of \ob and \at.
In our application, $\inci$ associates a word $w\in\ob$ to its
the indices of its nonzero sparse coding coordinates $i\in\at$.
% It is customary to represent the context as a cross table, where the
% row(column) headings are the object(attribute) names.  For each pair (g, m) ∈
% \inci, the corresponding cell in the cross table has an “×”.
FCA finds formal \emph{concept}, pairs \oaconc of object sets and attribute sets
($O\subseteq \ob,A \subseteq \at$) such that $A$
% We define the prime operator $'$ both for objects and attributes in a dual
% way: $O'$ is defined as $\{ a\in \at\mid \forall o\in O, \langle o,a \rangle
% \in \inci\}$ i.e.~$O'$
consists of the shared attributes of objects in $O$ (and no more); and
$O$ consists of
% $A'$ is defined as $\{ o\in \ob\mid \forall a\in A, \langle o,a \rangle \in
% \inci\}$ i.e.~$A'$
the objects in $\ob$ that have all the attributes in $A$ (and no more).
(There is a closure-operator related to each FAC context, for which $O$ and $A$
are closed sets iff \oaconc is a concept.)
% A formal concept in a context $C$ is a pair \oaconc~ such that $O' = A$ and $A' = O$.
$O$ is called the extent and $A$ is the intent of the concept.
% \footnote{
%  %IB(C) denotes the set of all concepts of the context C.
%  Those who are familiar with closure operators may note that the double
%  application of $'$ is a closure operation both on objects and attributes: with
%  notation $\bar S=S''$, for either $S\subseteq \ob$ or $S\subseteq \at$ we have
%  $S\subseteq \bar S$ and $\bar{\bar S}=S$, and the following conditions are
%  equivalent for all $O\subseteq \ob$ and $A\subseteq \at$:
%  \begin{itemize}
%    \item \oaconc~is a concept
%    \item $O$ is a closed set
%      %with respect to $O\mapsto\bar O$
%      and $A=O'$
%    \item $A$ is a closed set
%      %with respect to $A\mapsto\bar A$
%      and $O=A'$.
%  \end{itemize}
% }


There is an order defined in the context:
if $\langle A_1 , B_1 \rangle$ and $\langle A_2 , B_2 \rangle$ are concepts in
$C$, $\langle A_1 , B_1 \rangle$ is a \emph{subconcept} of $\langle A_2 , B_2
\rangle$ if $A_1 \subseteq A_2 $ which is equivalent to $B_1 \supseteq B_2 $.  In this case
%, $\langle A_2 , B_2 \rangle$ is a \emph{superconcept} of $\langle A_1 , B_1 \rangle$
%and
we write $\langle A_1 , B_1 \rangle \le \langle A_2 , B_2 \rangle$.
% The relation $\le$ is called the \emph{order} of the concepts.
% $C$ and
The concept order forms a %complete
lattice.
% The concept lattice of the context in Table 1, with full and reduced %
% figure % In-Close2 draws only reduced if I'm not mistaken.  labeling, is
% shown in \ref{fig:monkeylattice}.
% Full labeling means that a concept node is depicted with its full extent and
% intent. A reduced labeled concept lattice shows an object only in
The smallest (w.r.t.~$\le$) concept whose extent contains a word plays an
importand role in the concept hierarchy.  We say that this concept
% is called the object concept, or the concept that
introduces the object.
% Likewise, an attribute is shown only in the largest concept of whose intent
% the attribute is a member.%, the attribute concept, which introduces the
% attribute.

The closedness of extents and intents has an important consequence for
neuroscientific applications. Adding attributes to \at~(e.g.~responses of
additional neurons) will very probably grow the model.  However, the original
concepts will be embedded as a substructure in the larger lattice, with their
ordering relationships preserved.

\section{Our approach}

We use the popular skip-gram (SG) %and continuous-bag-of-words (CBOW)
approach \citep{Mikolov:2013f} to train $d=100$ dimensional dense distributed
word representations for each sub-corpora. The word embeddings are trained over the text corpora provided by the shared task organizers with the default training parameters of \texttt{word2vec} (w2v), i.e.~a window size of 10 and negative 25 negative samples for each positive contexts.

We derived \emph{multi-token units} by relying on the \texttt{word2phrase} software accompanying the w2v toolkit. An additional source for identifying multi-token units in the training corpus was the list of potential hypernyms released for each subtask by the shared task organizers.

For some subcorpus of the shared task $x\in\{1A, 1B, 1C, 2A, 2B\}$, we denote
the embedding matrix as $W_x \in \mathbb{R}^{d \times \lvert V_x \rvert}$ where
$\lvert V_x \rvert$ is the size of the vocabulary and $d$ is set to 100.
%as stated previously.

As a subsequent step we turn the dense vectorial word representations for subtask $x$ into
\emph{sparse word vectors} akin to \citet{Berend:2017} by solving for
\begin{equation}
\min\limits_{D \in \mathcal{C}, \alpha \in \mathbb{R}_{\geq0}} \lVert D\alpha - W_{x} \rVert_F + \lambda \lVert \alpha \rVert_1,
\label{nonneg_SPAMS_objective}
\end{equation}
where $\mathcal{C}$ refers to the convex set of $\mathbb{R}^{\lvert d \times k}$ matrices consisting of $d$-dimensional columns vectors
with norm at most $1$, and $\alpha$ contains the sparse coefficients for the elements of the vocabulary. The only difference
compared to \citet{Berend:2017} is that here we ensure a non-negativity
constraint over the elements of $\alpha$.

For the elements of the vocabulary we ran the \emph{formal concept analysis} tool of
\citet{Endres:2010}\footnote{\url{www.compsens.uni-tuebingen.de/pub/pages/personals/3/concepts.py}}.
In order to keep the size of the DAG outputted by the FCA algorithm manageable,
we only included the query words and those hypernyms in the analysis which
occur in the training dataset for the corpora.
As we will see in the next section, this restriction turns out to be very useful.

% official submission ('concept', '
%  cosines 8.8
%  is_frequent_hypernym 2.8
%  60_186 2.5
%  cand_is_first_w -2.4
%  textual_overlap 2.4
%  ...
% OFF OFF 1000
%  ...
%  same_first_w -1.0
%  att_intersect 0.8
%  diff_features -0.44
%  norm_ratios 0.27
%  ...
%  freq_ratios_log 0.1
%  att_diffB -0.06
% OFF OFF 200 ('concept'
%  ...
%  same_first_w -1.0
%  att_intersect 0.62
%  diff_features -0.43
%  norm_ratios 0.28
%  ...
%  freq_ratios_log 0.087
%  att_diffB -0.03

\begin{table}
	\begin{tabular}{lc}
    \toprule
		Core feature name   & Formula \\
    \midrule
		{cosine}            & $\frac{\mathbf{q}^\intercal \mathbf{h}}{\lVert \mathbf{q} \rVert_2\lVert \mathbf{h} \rVert_2}$ \\ % 1
		{difference}        & $\lVert \mathbf{q} - \mathbf{h} \rVert_2$ \\ % 8
		{normRatio}         & $\frac{\lVert \mathbf{q}\rVert_2}{\lVert \mathbf{h} \rVert_2}$ \\ % 9
    \midrule
    attributePair$_{ij}$      & $\langle i,j\rangle\in\phi(q)\times\phi(h)$ \\ % 3..
		{overlappingBasis}  & $\phi(q) \cap \phi(h) \neq \emptyset$ \\ % 7
		{sparseDifference$_{q\setminus h}$} & $\lvert \phi(q) - \phi(h) \rvert$ \\ % 11
		{sparseDifference$_{h\setminus q}$} & $\lvert \phi(h) - \phi(q) \rvert$ \\
    \midrule
		{qureyBeginsWith}   & $Q[0] = h$ \\ % 4
		{queryEndsWith}     & $Q[-1] = h$ \\
		{hasCommonWord} & $Q \cap H \neq \emptyset$ \\ % 5
		{sameFirstWord}        & $Q[0] = H[0]$ \\ % 6
		{sameLastWord}        & $Q[-1] = H[-1]$ \\
		{logFrequencyRatio} & $\log_{10}\frac{count(q)}{count(h)}$ \\ % 10
    \bottomrule
    % TODO vegyük be az fca-sokat is?  mert az att_pairs is bekerült
    % TODO illetve az is_frequent_hypernym (plusz esetleg ott is
    % megcsillagozni, hogy a beküldés pillanatában nem úgy működött mint ahogy
    % kellett volna neki)
	\end{tabular}
	\caption{The list of core features considering a $(q,h)$ pair of phrases.}
	\label{table:core_features}
  % not in this tabular: is_frequent_hypernym
\end{table}

Next, we determine a handful of features for a pair of expressions $(q, h)$
consisting of a query $q$ and its potential hypernym $h$.
Table~\ref{table:core_features} provides an overview of some of the features
employed for a pair $(q, h)$.
We denote with $\mathbf{q}$ and $\mathbf{h}$ the 100-dimensional dense
vectorial representations of $q$ and $h$.
Additionally, we refer to the set of basis vectors which are assigned nonzero
weights in the reconstruction of the vectorial representation of $q$ and $h$ as
$\phi(q)$ and $\phi(h)$.
Finally, we denote with $Q$ and $H$ the sequence of tokens constituting the
query and hypernym phrases.
%nekem kicsit furcsa nagy betűt használni erre. Nem jelölhetnék a kisbetűk
%eleve ezt?

Irrespective of the query, it is also considered as a feature whether a
particular candidate hypernym $h$ belongs to the top-50 most frequent hypernyms
for the category of $q$ (concept or entity, \texttt{isFreqHyp}).
% TODO legkésőbb itt belengetni a categoryánkénti bonáts jelentőségét
Three additional features are defined for incorporating the concept lattice
output by FCA, whether the most specific location within the DAG for $q$
\begin{itemize}
	\item coincides with that of $h$,
	\item is the parent for that of $h$,
	\item is the child for that of $h$.
\end{itemize}
% TODO ha már az FCA rész elnyeri a végső formáját: érthető-e ennyiből (most
% specific)
We will see in post-evaluation ablation experiments, where we refer to the
above three features as the \emph{FCA} features, that they were not useful in
our submissions.

\texttt{attributePair}$_{ij}$ above are indicator features for every possible
\emph{interaction term}
% esetleg a jelentőségét itt is kiemelni
between the sparse coefficients in $\alpha$. That means that for a pair of
words $(q, h)$ we defined $\phi(q) \times \phi(h)$, i.e.~candidates
get assigned with the Descartes product derived from the indices of the nonzero
coefficients in $\alpha$. Note that this feature template induces $k^2$
features, with $k$ being the number of basis vectors introduced in the dictionary matrix $D$ according to Eq.~\ref{nonneg_SPAMS_objective}.

In order to be able to rank potential candidates for previously unseen test
query cases we trained a separate \emph{logistic regression} classifier for the
\textit{concept} and \textit{entity} types of queries relying on the
\texttt{sklearn} package with the regularization parameter defaulting to $1.0$.
For each appropriate $(q,h)$ pair of words for which
$h$ is a hypernym of $q$, we generated a number of \emph{negative samples} $(q, h')$,
such that the training data does not include $h'$ as a valid hypernym for $q$.
For a given query $q$, belonging to either of the \textit{concept} or
% TODO concept, entity concept casinget egységesít
\textit{entity} category, we sampled $h'$ from those hypernyms which were
included as a valid hypernym in the training data with respect to some $q' \neq
q$ query phrase. The default number of negative samples ($ns$) generated for
each
positive training sample is 50. We also run post evaluation experiments in
which we regarded all the valid hypernyms in the training set -- not being a
proper hypernym for $q$ -- as $h'$. We refer to such a setting as $ns=all$ in
Section~\ref{sec:results}.

When making predictions for the hypernyms of a query, we relied on our query
type sensitive logistic regression model to determine the ranking of the
hypernym candidates. In our original submission we decided to treat such
phrases to rank for potentially being a proper hypernym which served as a
proper hypernym of at least one times on the training data for a given query
type. After the appropriate model ranked the hypernym candidates, we selected
the top 15 ranked candidates and applied a post-ranking heuristic over them,
i.e.~reordered them according to their background frequency from the training
corpus. Our assumption here is that more frequent words tend to refer to more
general concepts and more general hypernymy relations potentially tend to be
more easily detectable than more specialized ones.

\section{Results} \label{sec:results}

\subsection{Our submissions}

Our submissions were based on 200-dimensional sparse vectors computed from
unit-normed 100-dimensional dense vectors with $\lambda=.3$. The sum of the two
dimensions motivates our group name. For training the regression model with
negative sampling, 50 false hypernyms were sampled for each training query. One
of our submissions involved the interaction terms, the other not. Both
submissions used the conceptually motivated but practically harmful FCA-based
features.

\autoref{table:submissions} shows submission results.
The figures that can be achieved with the code in the project repo
(\texttt{reprd}) is slightly different from our official submissions
(\texttt{offic}) for two reasons:
because the implementation of \texttt{isFreqHyp} contained a bug, and because
of the natural randomness in negative sampling.
For reproducability, we report result without the \texttt{isFreqHyp} feature.
The randomness introduced by negative sampling is now factored out by random
seeding.

\begin{table*}
\resizebox{\textwidth}{!}{%
  \begin{tabular}{cc|cccccc|cccccc}
	& & \multicolumn{6}{c|}{without attribute pairs} & 	
\multicolumn{6}{c}{with attribute pairs}\\
\toprule
& &     MAP &    MRR &    P@1 &     P@3 &    P@5 &    P@15 & 	MAP 
&    MRR &    P@1 &    P@3 &   P@5 &   P@15 \\
\midrule
    1A	&	offic	&	8.6	&	18.0	&	13.0	&	8.9	&	8.2	&	7.9	&	
    8.9	&	19.4	&	14.9	&	9.3	&	8.6	&	8.1	\\
    1A	&	reprd	& 9.07 &18.7 &13.5	& 9.4 & 8.8 & 8.5 & 9.2	& 19.9	& 14.9	
    & 9.5	& 8.7	& 8.4	\\
    1B	&	offic	&	9.4	&	19.9	&	13.2	&	9.5	&	9.3	&	8.8	& 
    12.1	& 25.1	&	17.6	&	12.9	&	11.7	&	11.2	\\
    1B	&	reprd	& 9.2 & 19.5 & 12.8 & 8.9 & 8.9 & 8.7 & 12.8	& 26.7	& 
    18.9 & 13.6	& 12.4 & 11.9 \\
    1C	&	offic	&	12.5	&	25.9	&	16.6	&	13.6	&	12.6	
    &	11.5	& 17.9	&	37.6	&	27.8	&	19.7	&	17.1	&	
    16.3	\\
    1C	&	reprd	& 12.9	& 26.0	& 16.2	& 13.9	& 13.0	& 11.9	& 18.3	
    & 38.4	& 28.5	& 20.2	& 17.4	& 16.6	\\
    2A	&	offic	&	15.0	&	32.2	&	24.8	&	17.7	&	15.8	
    &	11.6	& 20.8	&	40.6	&	31.6	&	23.5	&	21.4	&	
    17.1	\\
    2A	&	reprd	&15.1	&32.4	& 24.4	&18.0	&16.2	&11.8	&21.5	
    &43.7	&35.6	&25.3	&21.8	&17.0	\\
    2B	&	offic	&	19.1	&	36.7	&	27.2	&	23.0	&	20.1	
    &	15.4	&29.5	&	46.4	&	33.0	&	31.9	&	28.9	&	
    27.7	\\
    2B	&	reprd	&21.5	&40.9	&29.6	&25.6	&22.1	&18.0	&30.4	
    &46.8	&33.8	&31.8	&29.5	&28.9	\\
    \bottomrule
  \end{tabular}
 }
  \caption{Our submissions results.}
  \label{table:submissions}
\end{table*}

% TODO Feaure weights and histograms 

%\subsection{Qualitative analysis} %TODO ?  entity music, Spanish (MRR, p@1)

% TODO \subsection{Generality, frequency and vector length}

\subsection{Query type sensitive baselining}

Our submission with attribute pairs achieved first place in the category 2B of
music entities. This is in part due to our good chioice of a fallback solution
in the case of OOV queries: we applied a category-sensitive baseline returning
the most frequent train hypernym in the corresponding query type (concept or
entity).  Table~\ref{table:coverage} shows how frequently we had to rely on
this fallback, and \autoref{table:MFH} shows the corresponding baseline
results.

\begin{table*}
	\centering
      %\resizebox{\textwidth}{!}{%
	\begin{tabular}{r|cccc|cccc}
    \toprule
		& \multicolumn{4}{c|}{concept} & \multicolumn{4}{c}{entity} \\
    & \multicolumn{2}{c}{Train} &
     \multicolumn{2}{c|}{Test} &
     \multicolumn{2}{c}{Train} &
     \multicolumn{2}{c}{Test} \\
     \midrule
1A & 975(4) & 0.41\%	&1055(4) & 0.38\%	&379(142) & 37.47\%	&344(99) & 28.78\% \\
1B & 709(1) & 0.14\%	&767(2) & 0.26\%	&249(41) & 16.47\%	&205(26) & 12.68\% \\
1C & 776(2) & 0.26\%	&625(2) & 0.32\%	&184(38) & 20.65\%	&328(45) & 13.72\% \\
2A & 442(58) & 13.12\%	&433(67) & 15.47\%	&0(0) & 	& 0(0) & \\
2B & 366(21) & 5.74\%	&341(17) & 4.99\%	&79(34) & 43.04\%	&102(40) & 39.22\% \\
\bottomrule
	\end{tabular}
  %}
  \caption{Number of in-vocabulary (and out-of-vocabulary, OOV) queries per query
  types.  The ratio of the latter is also shown.}
\label{table:coverage}
\end{table*}

\begin{table}
	\centering
      \resizebox{.5\textwidth}{!}{%
	\begin{tabular}{c|lllll}
    \toprule
    &  MAP  &  MRR  &  P@1  &  P@3 & P@5 \\
       \midrule
       % MFH: ..@15
    1A  & 9.8\% &0.216 & 19.8\% & 10.0\% & 9.0\% \\
    1A  & 8.77\% &0.2139 & 19.8\% & 8.91\% & 7.81\% \\ % & 7.53 \\
    \midrule
    1B  & 8.9\% &0.212 & 17.1\% & 9.1\% & 8.3\% \\
    1B  & 7.76\% &0.1937 & 17.1\% & 8.32\% & 6.82\% \\ % & 6.51 \\
    \midrule
    1C  & 16.4\% &0.333 & 24.6\% & 17.5\% & 16.1\% \\
    1C  & 12.16\% &0.2976 & 24.6\% & 11.98\% & 11.26\% \\ % & 11.04 \\
    \midrule
    2A  & 29.0\% &0.359 & 32.6\% & 34.3\% & 34.2\% \\
    2A  & 28.93\% &0.358 & 32.6\% & 34.27\% & 34.2\% \\ % & 21.39 \\
    \midrule
    2B  & 40.2\% &0.588 & 50.6\% & 44.6\% & 40.3\% \\
    2B  & 33.32\% &0.5148 & 36.2\% & 40.07\% & 35.76\% \\ % & 28.44 \\
    \bottomrule
  \end{tabular}
  }
  \label{table:MFH}
  \caption{Baseline results, most frequent training hypernyms. We (upper)
  consider the most frequent hypernym in the given query type (concept or
  entity). For comparison, we also show the \texttt{MFH} baseline provided by
  the organizers (lower) that is based on the most frequent hypernyms in
  general.}
\end{table}

\subsection{Post-evaluation analysis}

After the evaluation closed, we conducted ablation experiments, see
\autoref{table:ablation1A} and 
tried more settings including longer, 1000-dimensional sparse vectors, see
\autoref{table:1A_detailed}.

\input{ablation1A}

\input{post_eval}

\input{big_table}
% tabular 6 winner TODO

\subsection{Cantdidate restriction}

It turns out from \autoref{table:1A_detailed} that it is was useful in this
shared task to restrict the set of possible hypernyms to those in the training
set.  Table~\ref{table:upper} lists the results an oracle would achieve with
this restriction. Historically we applied this restriction to speed up the FCA
algorithm.

\begin{table}
	\centering
	\resizebox{.5\textwidth}{!}{
	\begin{tabular}{c|cccccc}
		   & MAP & MRR & P@1  & P@3 & P@5 & P@15 \\ \hline
		1A & 76.1 & 92.2 & 92.2 & 82.3 & 76.4 & 71.6 \\
		1B & 71.2 & 93.4 & 93.4 & 78.5 & 70.9 & 65.7 \\
		1C & 81.0 & 95.9 & 95.9 & 87.2 & 81.7 & 76.4 \\
		2A & 72.6 & 89.6 & 89.6 & 81.0 & 75.3 & 64.1 \\
		2B & 95.4 & 98.8 & 98.8 & 97.3 & 96.0 &	93.7 \\
	\end{tabular}
	}
  \caption{Results of an oracle restricted to the training hypernyms.}
	\label{table:upper}
\end{table}

%\section{Conclusion}

\bibliography{../../paper/Common/Bib/ml}
\bibliographystyle{acl_natbib}

\end{document}
