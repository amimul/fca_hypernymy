%% Based on the style files for NAACL-HLT 2018, which were

\documentclass[11pt,a4paper]{article}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

% To debug "\pdfendlink ended up in different nesting level than \pdfstartlink"
\usepackage[nohyperref]{naaclhlt2018}
\usepackage[draft]{hyperref}
%\usepackage{hyperref}

\usepackage{times}
\usepackage{latexsym}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
%\usepackage{amsmath}

\aclfinalcopy % Uncomment this line for all SemEval submissions

\setlength\titlebox{3in}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\title{300-sparsians at SemEval-2018 Task 9: \\
Hypernymy as interaction of sparse attributes}
%Employing Sparse Word Representations for Hypernym Discovery}
% TODO dorp "Employing"

\author{Gábor Berend \\
Department of Informatics \\ University of Szeged \\
Árpád tér 2, H6720 Szeged, Hungary \\
{\tt berendg@inf.u-szeged.hu} \\\And
  Márton Makrai  \\
  Institute for Linguistics \\
  Hungarian Academy of Sciences \\
  Benczúr u. 33, H1068 Budapest, Hungary \\
  {\tt makrai.marton@nytud.mta.hu} \\\AND
  Péter Földiák \\
  Secret Sauce Partners, \\
  657 Mission Suite 410, \\
  San Francisco CA 94105 \\
  {\tt Peter.Foldiak@gmail.com} \\}

\date{}

\begin{document}

\maketitle

\begin{abstract}
  We apply a concept hierarchy obtained from a word embedding by combining the
  methods of sparse coding and formal concept analysis (concept lattices).
  Our experiments place first in four subtasks (e.g.~the music entities
  subtask).
\end{abstract}

% bázisok alapján épített fca-val akartuk megoldani az egészet, de aztán azt
% láttuk h ez önmagában nem lenne elég, mert a bázisok nem viselkedtek olyan
% szépen, mint amennyire kellett volna nekik ahhoz, hogy az fca-ból
% kipottyanjon a fogalmi hierarchia

% a potenciális hipernímákat jelentősen meg kellett szűrni ahhoz h az fca-n
% gyorsítani tudjunk ezáltal viszont a hipernímaként való rangsorolás
% tekintetében is egy szűkített halmazzal dolgoztunk a szervezők által
% kiadottakhoz képest
% ami miatt 0.5 körüli MAP-nál jobbat elvileg sem érhettünk volna el

% \item sűrű vektoros és felszíni jegyek
% \item ritka reprezentációból származtatott descartes szorzatos jegyeket
% \item ablációs kísérletek erre (descaretes on/off)
% \item a tanult súlyokról lehetne még írni

\section{Introduction}

% röviden, mondjuk úgy 2-3 mondatban le kéne írni a shared task feladatát
For further details of the shared task see \citet{Camacho-Collados:2018}.

Natural language phenomena are extremely sparse by their nature, whereas
continuous word embeddings employ dense representations of words. Turning
these dense representations into a much sparser form can help in focusing on
most salient parts of word representations
\citep{Faruqui:2015,Berend:2017,Subramanian:2018}.

Sparse representation is related to hypernymy in natural ways.
One formulation is hierarchical sparse coding \citep{Zhao:2009}, where trees
describe the order in which variables “enter the model” (i.e., take nonzero
% TODO nonzero vs non zero vs non-zero legvégül egységesít
values). A node may take a nonzero value only if its ancestors also do: the
dimensions that correspond to top level nodes should focus on “general”
contexts that are present in most words. \citet{Yogatama:2015} offer an
implementation that is efficient for gigaword corpora.

Here we illustrate the effectiveness of sparse coding derived word representations in the task of hypernymy extraction. Additionally, we employ sparse word representations in a different way, namely through Formal Concept Analysis (FCA).
Our source code in available at \url{https://github.com/begab/fca_hypernymy}.

% nonneg \citep{Faruqui:2015,Fyshe:2015}
% a SPINE idézi Danish+ 16-ot, de az önhivatkozás

%entities, music, Spanish

%a task kiírásában levő irodalom

\subsection{Formal concept analysis}

\newcommand{\ob}{\ensuremath{\mathcal O}}
\newcommand{\at}{\ensuremath{\mathcal A}}
\newcommand{\inci}{\ensuremath{\mathcal I}}
\newcommand{\oaconc}{\ensuremath{\langle O, A\rangle}}

Formal Concept Analysis (FCA) is the mathematization of \emph{concept} and conceptual
hierarchy \citep{Ganter:2012,Endres:2010}. % vagy 2009
An FCA context $C = \langle \ob, \at, \inci\rangle$ is comprised of
a set of \emph{objects} $\ob$, a set of \emph{attributes} $\at$ and
a binary incidency relation $\inci \subseteq \ob \times \at$
between members of \ob and \at.
In our application, the members of $\ob$ are words, whereas
the members of $\at$ are the sparse coding coordinate indices.
Relation $\inci$ contains a pair $\langle o, a\rangle$
if the $a$th coordinate of the sparse vector for word $o \in \ob$ is non-zero,
% It is customary to represent the context as a cross table, where the
% row(column) headings are the object(attribute) names.  For each pair (g, m) ∈
% \inci, the corresponding cell in the cross table has an “×”.
% TODO example
We define the prime operator $'$ both for objects and attributes in a dual way:
given $O\subseteq \ob$ and $A \subseteq \at$,
$O'$ is defined as $\{ a\in \at\mid \forall o\in O, \langle o,a \rangle \in \inci\}$
i.e.~$O'$ contains the shared attributes of objects in $O$; and
$A'$ is defined as $\{ o\in \ob\mid \forall a\in A, \langle o,a \rangle \in \inci\}$
i.e.~$A'$ contains the objects in $\ob$ that have all the attributes in $A$.
A formal concept in a context $C$ is a pair \oaconc~ such that $O' = A$ and $A' = O$.
$O$ is called the extent and $A$ is the intent of the concept.\footnote{
  %IB(C) denotes the set of all concepts of the context C.
  Those who are familiar with closure operators may note that the double
  application of $'$ is a closure operation both on objects and attributes: with
  notation $\bar S=S''$, for either $S\subseteq \ob$ or $S\subseteq \at$ we have
  $S\subseteq \bar S$ and $\bar{\bar S}=S$, and the following conditions are
  equivalent for all $O\subseteq \ob$ and $A\subseteq \at$:
  \begin{itemize}
    \item \oaconc~is a concept
    \item $O$ is a closed set
      %with respect to $O\mapsto\bar O$
      and $A=O'$
    \item $A$ is a closed set
      %with respect to $A\mapsto\bar A$
      and $O=A'$
  \end{itemize}
}
For a lattice representation of the relationships between concepts, one defines
an order on $C$:
if $\langle A_1 , B_1 \rangle$ and $\langle A_2 , B_2 \rangle$ are concepts in
$C$, $\langle A_1 , B_1 \rangle$ is a \emph{subconcept} of $\langle A_2 , B_2
\rangle$ if $A_1 \subseteq A_2 $ which is equivalent to $B_1 \supseteq B_2 $.  In this case
%, $\langle A_2 , B_2 \rangle$ is a \emph{superconcept} of $\langle A_1 , B_1 \rangle$
%and
we write $\langle A_1 , B_1 \rangle \le \langle A_2 , B_2 \rangle$.
% The relation $\le$ is called the \emph{order} of the concepts.

$C$ and the concept order form a %complete
lattice.  The concept lattice of the context in Table 1, with full and reduced
% TODO figure
% In-Close2 draws only reduced if I'm not mistaken.
labeling, is shown in \ref{fig:monkeylattice}.
Full labeling means that a concept node is depicted with its full extent and
intent. A reduced labeled concept lattice shows an object only in the smallest
(w.r.t.~$\le$) concept of whose extent the object is a member.
% This concept is called the object concept, or the concept that introduces the
% object.
Likewise, an attribute is shown only in the largest concept of whose intent the
attribute is a member.%, the attribute concept, which introduces the attribute.
% The closedness of extents and intents has an important consequence for
% neuroscientific applications. Adding attributes to \at (e.g. responses of
% additional neurons) will very probably grow IB(C). However, the original
% concepts will be embedded as a substructure in the larger lattice, with their
% ordering relationships preserved.

\section{Related work}

The idea of acquiring concept hierarchies from a text corpus with the tools of
Formal Concept Analysis (FCA) is relatively new \citep{Cimiano:2005}.

\section{Our submissions}

We use the popular skip-gram (SG) %and continuous-bag-of-words (CBOW)
approach \citep{Mikolov:2013f} to train $d=100$ dimensional dense distributed
word representations for each sub-corpora. The word embeddings are trained over the text corpora provided by the shared task organizers with the default training parameters of \texttt{word2vec} (w2v), i.e.~a window size of 10 and negative 25 negative samples for each positive contexts.

We derived multi-token units by relying on the \texttt{word1phrase} software accompanying the w2v toolkit. An additional source for identifying multi-token units in the training corpus was the list of potential hypernyms released for each subtask by the shared task organizers.

For some subcorpus of the shared task $x\in\{1A, 1B, 1C, 2A, 2B\}$, we denote
the embedding matrix as $W_x \in \mathbb{R}^{d \times \lvert V_x \rvert}$ where
$\lvert V_x \rvert$ is the size of the vocabulary and $d$ is set to 100.
%as stated previously.

As a subsequent step we turn the dense vectorial word representations for subtask $x$ into
sparse word vectors akin to \citet{Berend:2017} by solving for
\begin{equation}
\min\limits_{D \in \mathcal{C}, \alpha \in \mathbb{R}_{\geq0}} \lVert D\alpha - W_{x} \rVert_F + \lambda \lVert \alpha \rVert_1,
\label{nonneg_SPAMS_objective}
\end{equation}
where $\mathcal{C}$ refers to the convex set of $\mathbb{R}^{\lvert d \times k}$ matrices consisting of $d$-dimensional columns vectors
with norm at most $1$, and $\alpha$ contains the sparse coefficients for the elements of the vocabulary. The only difference
compared to \citet{Berend:2017} is that here we ensure a non-negativity
constraint over the elements of $\alpha$.

For the elements of the vocabulary we ran the formal concept analysis tool of
\citet{Endres:2010}\footnote{\url{www.compsens.uni-tuebingen.de/pub/pages/personals/3/concepts.py}}. In order to keep the size of the DAG outputted by the FCA algorithm manageable, we only included the query words and those hypernyms in the analysis which occur in the training dataset for the corpora.
% már itt említeni hogy a postevalban máshogy is lesz

Next, we determine a handful of features for a pair of expressions $(q, h)$
consisting of a query $q$ and its potential hypernym $h$. We denote with $Q$
and $H$ the sequence of tokens constituting the query and hypernym phrases.
%nekem kicsit furcsa nagy betűt használni erre. Nem jelölhetnék a kisbetűk
%eleve ezt? Úgy látom, egyelőre nem használod később nagybetűs jelölést.
Additionally, we refer to the set of basis vectors which are assigned non-zero weights in the reconstruction of the vectorial representation of $q$ and $h$ as $\phi(q)$ and $\phi(h)$. Finally, we denote with $\mathbf{q}$ and $\mathbf{h}$ the 100-dimensional dense vectorial representations of $q$ and $h$.

Table~\ref{table:core_features} provides an overview of the features employed
for a pair $(q, h)$. Additionally, it is also considered as a feature whether a
particular candidate hypernym $h$ belongs to the top-50 most frequent hypernyms
for the category of $q$ (concept or entity). 
% legkésőbb itt belengetni a categoryánkénti bonáts jelentőségét
Three additional optional features are defined for incorporating the concept
tree output by FCA, whether the most specific location within the DAG for $q$
% vagy more subtle? (more optional-t írtál)
% nekem fura az i.e. ebben a jelentésben -- de ezeket a finomságokat persze
% elég a végső beadáskor megbeszélnünk
\begin{enumerate}
	\item coincides with that of $h$,
	\item is the parent for that of $h$,
	\item is the child for that of $h$.
\end{enumerate}
% TODO ha már az FCA rész elnyeri a végső formáját: érthető-e ennyiből (most
% specific)
During our ablation experiments, we refer to the above three features as the \emph{FCA} features.

Finally, we define one indicator feature for every possible interaction term
% esetleg a jelentőségét itt is kiemelni
between the sparse coefficients in $\alpha$. That means that for a pair of
words $(q, h)$ we defined $\Phi(q,h)=\phi(q) \times \phi(h)$, i.e.~candidates
get assigned with the Descartes product derived from the indices of the nonzero
coefficients in $\alpha$. Note that this feature template induces $k^2$
features, with $k$ being the number of basis vectors introduced in the dictionary matrix $D$ according to Eq.~\ref{nonneg_SPAMS_objective}.

In order to be able to rank potential candidates for previously unseen test
query cases we trained a logistic regression classifier relying on the
\texttt{sklearn} package. For each appropriate $(q,h)$ pair of words for which
$h$ is a hypernym of $q$, we generated a number of negative samples $(q, h')$,
such that the training data does not include $h'$ as a valid hypernym for $q$.
For a given query $q$, belonging to either of the \textit{Concept} or
% TODO Concept, Entity Concept casinget egységesít
\textit{Entity} category, we sampled $h'$ from those hypernyms which were
included as a valid hypernym in the training data with respect to some $q' \neq
q$ query phrase. The default number of negative samples generated for each
positive training sample is 50. 
% Ha beleírjuk az 5000-es dolgot is, akkor itt lehet utalni rá, hogy lásd
% post-eval eredmények.

\begin{table}
	\begin{tabular}{ll}
		Core feature name   & Short description \\  \hline
		$difference$        & $\lVert \mathbf{q} - \mathbf{h} \rVert_2$ \\
		$normRatio$         & $\frac{\lVert \mathbf{q}\rVert_2}{\lVert \mathbf{h} \rVert_2}$ \\
		$cosine$            & $\frac{\mathbf{q}^\intercal \mathbf{h}}{\lVert \mathbf{q} \rVert_2\lVert \mathbf{h} \rVert_2}$ \\
		$logFrequencyRatio$ & $\log_{10}\frac{count(q)}{count(h)}$ \\
		$hasTextualOverlap$ & $Q \cap H \neq \emptyset$ \\
		$samePrefix$        & $Q[0] = H[0]$ \\
		$sameSuffix$        & $Q[-1] = H[-1]$ \\
		$qureyBeginsWith$   & $Q[0] = h$ \\
		$queryEndsWith$     & $Q[-1] = h$ \\
		$sparseDifference_{qh}$ & $\lvert \phi(q) - \phi(h) \rvert$ \\
		$sparseDifference_{hq}$ & $\lvert \phi(h) - \phi(q) \rvert$ \\
		$overlappingBasis$  & $\phi(q) \cap \phi(h) \neq \emptyset$ \\
	\end{tabular}
	\caption{The list of core features considering a $(q,h)$ pair of phrases.}
	\label{table:core_features}
\end{table}


% why 300 in the group name (akár ott, hogy 300-as ritkával jobb is lehetett
% volna, vagy ott, hogy 200 + 100 -- de ha magától értődő, akkor
% ne)

\section{Experimental results}

\begin{table}
	\centering
	\begin{tabular}{c|ccccc}
		   &  MAP  &  MRR  &  P@1  &  P@3  & P@10  \\ \hline
		1A & 0.098 & 0.226 & 0.198 & 0.100 & 0.090 \\
		1B & 0.089 & 0.212 & 0.171 & 0.091 & 0.083 \\
		1C & 0.164 & 0.333 & 0.246 & 0.175 & 0.161 \\
		2A & 0.290 & 0.359 & 0.326 & 0.343 & 0.342 \\
		2B & 0.402 & 0.588 & 0.506 & 0.446 & 0.403 \\
	\end{tabular}
	\label{table:MFH}
	\caption{Results of our query type sensitive most frequent hypernym baseline.}
\end{table}

Table~\ref{table:coverage} shows the coverage of our vocabulary with respect to
the query words on the training and test datasets of the various subtasks.

\begin{table}
	\centering
	\begin{tabular}{r|cc|cc}
		& \multicolumn{2}{c|}{Concept} & \multicolumn{2}{c}{Entity} \\
		& Train & Test   & Train & Test \\ \hline
	1A  & 975/4 & 1055/4 & 379/142 & 344/99 \\
	1B  & 709/1 & 767/2  & 249/41  & 205/26 \\
	1C  & 776/2 & 625/2  & 184/38  & 328/45 \\
	2A  & 442/58& 433/67 & 0/0     & 0/0    \\
	2B  & 366/21& 341/17 & 79/34   & 102/40 \\
	\end{tabular}
\caption{Number of in-vocabulary/out-of-vocabulary queries per query types.}
\label{table:coverage}
\end{table}


\begin{table}
	\centering
	\begin{tabular}{r|ccccc}
att-pairs/fca & MAP & MRR & P@1 & P@3 \\ \hline
off/off & 0.103 & 0.213 & 0.150 & 0.106 \\
off/on  & 0.101 & 0.211 & 0.149 & 0.105 \\
on/off  & 0.121 & 0.254 & 0.189 & 0.129 \\
on/on   & 0.121 & 0.253 & 0.187 & 0.130 \\
	\end{tabular}
\caption{Ablation experiments for the 1A dataset.}
\label{table:ablation1A}
\end{table}

\subsection{Restricted cantdidates}

Table~\ref{table:upper} lists the best possible results we can achieve.

\begin{table}
	\centering
	\begin{tabular}{c|cccc}
		   & MAP & MRR & P@1  & P@3 \\ \hline
		1A & 0.761 & 0.922 & 0.922 & 0.823 \\
		1B & 0.712 & 0.934 & 0.934 & 0.785 \\
		1C & 0.810 & 0.959 & 0.959 & 0.872 \\
		2A & 0.726 & 0.896 & 0.896 & 0.810 \\
		2B & 0.954 & 0.988 & 0.988 & 0.973 \\
	\end{tabular}
	\label{table:upper}
	\caption{Best possible results obtainable on the test set for our
submissions due to the filtration of the possible set of gold hypernyms
we apply in order to speed up the FCA algorithm we use.}
\end{table}

entity length ratio figure %TODO

\subsection{All provided cantdidates}

weights of dense and surface features %TODO

\subsection{Qualitative analysis} %TODO ?

entity music, Spannish (MRR, p@1)

\section{Conclusion}

\bibliography{../../paper/Common/Bib/ml}
\bibliographystyle{acl_natbib}

\end{document}
